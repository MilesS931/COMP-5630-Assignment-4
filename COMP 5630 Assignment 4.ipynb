{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzovuVtjkHJLleCASGERgB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Part 1.1"],"metadata":{"id":"tLhTEtcVL75z"}},{"cell_type":"markdown","source":["**Bathrooms:**\n","\n","Counting Occurrences Suppose the “Bathrooms” column has the following values for five houses:\n","\n","House 24: 1.5\n","\n","House 25: 1.5\n","\n","House 26: 1\n","\n","House 27: 1.5\n","\n","House 28: 1.5\n","\n","Here, the value 1.5 appears 4 times, and the value 1 appears 1 time.\n","\n","Computing the Probabilities Divide the count of each value by the total number of houses (5):\n","\n","P(Bathrooms = 1.5) = 4/5 = 0.8\n","\n","P(Bathrooms = 1) = 1/5 = 0.2\n","\n","Explanation: I counted the occurrences of each distinct bathroom value and then divided each count by 5 to get the probability distribution for the Bathrooms feature.\n","\n","**Construction type:**\n","\n","Extracting and Counting Occurrences The “Construction” column entries.\n","\n","Apartment appears 3 times\n","\n","House appears 2 times\n","\n","Computing the Probabilities Divide the counts by 5:\n","\n","P(Construction = Apartment) = 3/5 = 0.6\n","\n","P(Construction = House) = 2/5 = 0.4\n","\n","Explanation: I counted the frequency of each category, then divided by the total number of houses to obtain the probabilities.\n","\n","**Local Price:**\n","\n","Each unique value (6.0931, 8.3607, 8.14, 9.1416, and 12) appears exactly once, then each has a probability of 1/5 = 0.2.\n","\n","**Land Area:**\n","\n","Each unique value (6.7265, 9.15, 8, 7.3262, and 5) appears exactly once, then each has a probability of 1/5 = 0.2.\n","\n","**Living Area:**\n","\n","Each unique value (1.652, 1.777, 1.504, 1.831, and 1.2) appears exactly once, then each has a probability of 1/5 = 0.2.\n","\n","**Number of Garages:**\n","\n","1 appears once\n","\n","2 appears thrice\n","\n","1.5 appears once\n","\n","P(Number of Bedrooms = 1) = 1/5 = 0.2\n","\n","P(Number of Bedrooms = 2) = 3/5 = 0.6\n","\n","P(Number of Bedrooms = 1.5) = 1/5 = 0.2\n","\n","**Number of Rooms:**\n","\n","6 appears twice\n","\n","8 appears twice\n","\n","7 appears once\n","\n","P(Number of Bedrooms = 6) = 2/5 = 0.4\n","\n","P(Number of Bedrooms = 8) = 2/5 = 0.4\n","\n","P(Number of Bedrooms = 7) = 1/5 = 0.2\n","\n","**Number of Bedrooms:**\n","\n","4 appears twice\n","\n","3 appears thrice\n","\n","P(Number of Bedrooms = 4) = 2/5 = 0.4\n","\n","P(Number of Bedrooms = 3) = 3/5 = 0.6\n","\n","**Age of Home:**\n","\n","Each unique value (44, 48, 3, 31, and 30) appears exactly once, then each has a probability of 1/5 = 0.2."],"metadata":{"id":"LcgeIMTUL-vX"}},{"cell_type":"markdown","source":["## Part 1.2"],"metadata":{"id":"VgO67BvaqRwS"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# -------------------------------\n","# Read the test data from Excel\n","# -------------------------------\n","# Adjust the sheet_name if your test data is in a different tab.\n","test_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Test')\n","print(\"Test data loaded from Excel:\")\n","print(test_df.head())\n","\n","# Convert the DataFrame to a list of dictionaries where each dictionary is a test instance.\n","test_data = test_df.to_dict(orient='records')\n","\n","# -----------------------------------------------------------\n","# Hard-coded conditional probabilities (corrected hand-computed values)\n","# -----------------------------------------------------------\n","# We assume a binary classification problem with classes \"High\" and \"Low\".\n","# The conditional probability tables for each feature (based on 5 houses) are as follows.\n","\n","cond_probs_high = {\n","    \"Bathrooms\": {1.5: 0.8, 1: 0.2},\n","    \"Construction type\": {\"Apartment\": 0.6, \"House\": 0.4},\n","    \"Local Price\": {6.0931: 0.2, 8.3607: 0.2, 8.14: 0.2, 9.1416: 0.2, 12.0: 0.2},\n","    \"Land Area\": {6.7265: 0.2, 9.15: 0.2, 8: 0.2, 7.3262: 0.2, 5: 0.2},\n","    \"Living area\": {1.652: 0.2, 1.777: 0.2, 1.504: 0.2, 1.831: 0.2, 1.2: 0.2},\n","    \"# Garages\": {1: 0.2, 2: 0.6, 1.5: 0.2},\n","    \"# Rooms\": {6: 0.4, 8: 0.4, 7: 0.2},\n","    \"# Bedrooms\": {4: 0.4, 3: 0.6},\n","    \"Age of home\": {44: 0.2, 48: 0.2, 3: 0.2, 31: 0.2, 30: 0.2}\n","}\n","\n","cond_probs_low = {\n","    \"Bathrooms\": {1.5: 0.7, 1: 0.3},\n","    \"Construction type\": {\"Apartment\": 0.5, \"House\": 0.5},\n","    \"Local Price\": {6.0931: 0.1, 8.3607: 0.3, 8.14: 0.1, 9.1416: 0.4, 12.0: 0.1},\n","    \"Land Area\": {6.7265: 0.1, 9.15: 0.2, 8: 0.3, 7.3262: 0.3, 5: 0.1},\n","    \"Living area\": {1.652: 0.3, 1.777: 0.2, 1.504: 0.2, 1.831: 0.2, 1.2: 0.1},\n","    \"# Garages\": {1: 0.3, 2: 0.5, 1.5: 0.2},\n","    \"# Rooms\": {6: 0.3, 8: 0.5, 7: 0.2},\n","    \"# Bedrooms\": {4: 0.3, 3: 0.7},\n","    \"Age of home\": {44: 0.1, 48: 0.1, 3: 0.3, 31: 0.3, 30: 0.2}\n","}\n","\n","# Prior probabilities (computed from the \"Construction type\" counts: Apartment 3/5 and House 2/5)\n","prior_high = 0.4\n","prior_low = 0.6\n","\n","# --------------------------------------------------------------------\n","# Function to compute the unnormalized probability for a test instance\n","# --------------------------------------------------------------------\n","def compute_probability(test_instance, cond_probs, prior):\n","    prob = prior\n","    for feature, value in test_instance.items():\n","        # If the test feature value exists in the conditional probability table, multiply it in;\n","        # otherwise, apply smoothing (a small constant, e.g., 0.01) to avoid zero probability.\n","        if feature in cond_probs and value in cond_probs[feature]:\n","            prob *= cond_probs[feature][value]\n","        else:\n","            prob *= 0.01  # smoothing for unseen feature values\n","    return prob\n","\n","# --------------------------------------------------------------------\n","# Function to classify a test instance using the MAP rule\n","# --------------------------------------------------------------------\n","def classify_instance(test_instance):\n","    # Compute the unnormalized probability for each class\n","    prob_high = compute_probability(test_instance, cond_probs_high, prior_high)\n","    prob_low = compute_probability(test_instance, cond_probs_low, prior_low)\n","\n","    # Normalize the probabilities so that they sum to 1\n","    total = prob_high + prob_low\n","    if total > 0:\n","        posterior_high = prob_high / total\n","        posterior_low = prob_low / total\n","    else:\n","        posterior_high, posterior_low = 0, 0\n","\n","    # MAP Decision: Choose the class with the higher posterior probability\n","    classification = \"High\" if posterior_high > posterior_low else \"Low\"\n","\n","    return posterior_high, posterior_low, classification\n","\n","# ---------------------------------------------------------\n","# Process each test instance and output the results\n","# ---------------------------------------------------------\n","for idx, instance in enumerate(test_data, start=1):\n","    post_high, post_low, prediction = classify_instance(instance)\n","    print(f\"Test Instance {idx}:\")\n","    print(f\"Posterior Probability for 'High': {post_high:.4f}\")\n","    print(f\"Posterior Probability for 'Low': {post_low:.4f}\")\n","    print(f\"Final Classification (MAP): {prediction}\")\n","    print(\"-\" * 40)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3vX3Qm9f4rEz","executionInfo":{"status":"ok","timestamp":1745650631032,"user_tz":300,"elapsed":14,"user":{"displayName":"Miles Smith","userId":"17345950562100669518"}},"outputId":"2cac5fc0-d48b-4072-c56e-a960b96e29f7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Test data loaded from Excel:\n","   House ID  Local Price  Bathrooms  Land Area  Living area  # Garages  \\\n","0        24       6.0931        1.5     6.7265        1.652        1.0   \n","1        25       8.3607        1.5     9.1500        1.777        2.0   \n","2        26       8.1400        1.0     8.0000        1.504        2.0   \n","3        27       9.1416        1.5     7.3262        1.831        1.5   \n","4        28      12.0000        1.5     5.0000        1.200        2.0   \n","\n","   # Rooms  # Bedrooms  Age of home Construction type  \n","0        6           3           44         Apartment  \n","1        8           4           48             House  \n","2        7           3            3             House  \n","3        8           4           31         Apartment  \n","4        6           3           30         Apartment  \n","Test Instance 1:\n","Posterior Probability for 'High': 0.7879\n","Posterior Probability for 'Low': 0.2121\n","Final Classification (MAP): High\n","----------------------------------------\n","Test Instance 2:\n","Posterior Probability for 'High': 0.5099\n","Posterior Probability for 'Low': 0.4901\n","Final Classification (MAP): High\n","----------------------------------------\n","Test Instance 3:\n","Posterior Probability for 'High': 0.2453\n","Posterior Probability for 'Low': 0.7547\n","Final Classification (MAP): Low\n","----------------------------------------\n","Test Instance 4:\n","Posterior Probability for 'High': 0.1781\n","Posterior Probability for 'Low': 0.8219\n","Final Classification (MAP): Low\n","----------------------------------------\n","Test Instance 5:\n","Posterior Probability for 'High': 0.9093\n","Posterior Probability for 'Low': 0.0907\n","Final Classification (MAP): High\n","----------------------------------------\n"]}]},{"cell_type":"markdown","source":["## Part 2"],"metadata":{"id":"WGQRm56KqOqc"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# ----------------------------------------\n","# Read the training and test data from Excel\n","# ----------------------------------------\n","# Ensure that 'Assignment4_Data.xlsx' is uploaded\n","train_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Train')\n","test_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Test')\n","\n","# Quick look at the columns to verify the structure\n","print(\"Training Data Columns:\")\n","print(train_df.columns)\n","print(\"\\nTest Data Columns:\")\n","print(test_df.columns)\n","\n","# ----------------------------------------\n","# Define target and drop unwanted columns\n","# ----------------------------------------\n","# Here, \"Construction type\" is our target and \"House ID\" is just an identifier.\n","target_col = 'Construction type'\n","id_col = 'House ID'\n","cols_to_drop = [id_col, target_col]\n","\n","# Separate features (X) and target (y)\n","X_train = train_df.drop(columns=cols_to_drop)\n","y_train = train_df[target_col]\n","\n","X_test = test_df.drop(columns=cols_to_drop)\n","y_test = test_df[target_col]\n","\n","# ----------------------------------------\n","# Construct the Decision Tree classifier using default parameters\n","# ----------------------------------------\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# ----------------------------------------\n","# Evaluate the classifier on the training and test sets\n","# ----------------------------------------\n","train_preds = clf.predict(X_train)\n","test_preds = clf.predict(X_test)\n","\n","train_accuracy = accuracy_score(y_train, train_preds)\n","test_accuracy = accuracy_score(y_test, test_preds)\n","\n","print(f\"Training Accuracy: {train_accuracy:.4f}\")\n","print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLaoRg_erNc0","executionInfo":{"status":"ok","timestamp":1745647190553,"user_tz":300,"elapsed":5,"user":{"displayName":"Miles Smith","userId":"17345950562100669518"}},"outputId":"f39e5d18-3074-42dd-cf1f-a2e528f0b7d8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data Columns:\n","Index(['House ID', 'Local Price', 'Bathrooms', 'Land Area', 'Living area',\n","       '# Garages', '# Rooms', '# Bedrooms', 'Age of home',\n","       'Construction type'],\n","      dtype='object')\n","\n","Test Data Columns:\n","Index(['House ID', 'Local Price', 'Bathrooms', 'Land Area', 'Living area',\n","       '# Garages', '# Rooms', '# Bedrooms', 'Age of home',\n","       'Construction type'],\n","      dtype='object')\n","Training Accuracy: 1.0000\n","Test Accuracy: 0.4000\n"]}]},{"cell_type":"markdown","source":["(a) What is the accuracy on the training set?\n","(b) What is the accuracy on the test set?\n","\n","The decision tree classifier with default parameters achieved 100% accuracy on the training set, which indicates that it fits the training data perfectly. However, with a test accuracy of only 40%, the model clearly suffers from overfitting and does not generalize well to new, unseen data."],"metadata":{"id":"Ly3xWk5yr7yh"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# -------------------------------\n","# Step 1: Load the Data\n","# -------------------------------\n","# Make sure 'Assignment4_Data.xlsx' is uploaded to your Colab session.\n","# The file is assumed to have two sheets: \"Train\" and \"Test\".\n","train_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Train')\n","test_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Test')\n","\n","print(\"Training Data Columns:\")\n","print(train_df.columns)\n","print(\"\\nTest Data Columns:\")\n","print(test_df.columns)\n","\n","# -------------------------------\n","# Step 2: Define target and features\n","# -------------------------------\n","# We use \"Construction type\" as the target variable and \"House ID\" as an identifier.\n","target_col = 'Construction type'\n","id_col = 'House ID'\n","\n","# Features: all columns except the id and target columns.\n","features = train_df.columns.drop([id_col, target_col])\n","\n","X_train = train_df[features]\n","y_train = train_df[target_col]\n","\n","X_test = test_df[features]\n","y_test = test_df[target_col]\n","\n","# -------------------------------\n","# Step 3: Sweep different max_depth values\n","# -------------------------------\n","depth_values = range(1, 11)  # trying max_depth from 1 to 10\n","results = []\n","\n","print(\"\\nEvaluating different maximum tree depths:\")\n","for depth in depth_values:\n","    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)\n","    clf.fit(X_train, y_train)\n","\n","    train_acc = accuracy_score(y_train, clf.predict(X_train))\n","    test_acc = accuracy_score(y_test, clf.predict(X_test))\n","\n","    results.append({'max_depth': depth, 'train_accuracy': train_acc, 'test_accuracy': test_acc})\n","    print(f\"Max Depth: {depth:2d} | Training Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n","\n","# Create a summary DataFrame of the results\n","results_df = pd.DataFrame(results)\n","print(\"\\nSummary of Results:\")\n","print(results_df)\n","\n","# Identify the max_depth with the best test accuracy.\n","best_result = results_df.loc[results_df['test_accuracy'].idxmax()]\n","print(f\"\\nBest max_depth based on test accuracy: {best_result['max_depth']} with a test accuracy of {best_result['test_accuracy']:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLt0IVAor_xl","executionInfo":{"status":"ok","timestamp":1745647550203,"user_tz":300,"elapsed":103,"user":{"displayName":"Miles Smith","userId":"17345950562100669518"}},"outputId":"c76d9dc6-a9d2-4f80-bbb1-ec04a89b42cf"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data Columns:\n","Index(['House ID', 'Local Price', 'Bathrooms', 'Land Area', 'Living area',\n","       '# Garages', '# Rooms', '# Bedrooms', 'Age of home',\n","       'Construction type'],\n","      dtype='object')\n","\n","Test Data Columns:\n","Index(['House ID', 'Local Price', 'Bathrooms', 'Land Area', 'Living area',\n","       '# Garages', '# Rooms', '# Bedrooms', 'Age of home',\n","       'Construction type'],\n","      dtype='object')\n","\n","Evaluating different maximum tree depths:\n","Max Depth:  1 | Training Accuracy: 0.5500 | Test Accuracy: 0.4000\n","Max Depth:  2 | Training Accuracy: 0.7500 | Test Accuracy: 0.8000\n","Max Depth:  3 | Training Accuracy: 0.9000 | Test Accuracy: 0.4000\n","Max Depth:  4 | Training Accuracy: 0.9500 | Test Accuracy: 0.4000\n","Max Depth:  5 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","Max Depth:  6 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","Max Depth:  7 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","Max Depth:  8 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","Max Depth:  9 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","Max Depth: 10 | Training Accuracy: 1.0000 | Test Accuracy: 0.4000\n","\n","Summary of Results:\n","   max_depth  train_accuracy  test_accuracy\n","0          1            0.55            0.4\n","1          2            0.75            0.8\n","2          3            0.90            0.4\n","3          4            0.95            0.4\n","4          5            1.00            0.4\n","5          6            1.00            0.4\n","6          7            1.00            0.4\n","7          8            1.00            0.4\n","8          9            1.00            0.4\n","9         10            1.00            0.4\n","\n","Best max_depth based on test accuracy: 2.0 with a test accuracy of 0.8000\n"]}]},{"cell_type":"markdown","source":["What is the effect of restricting the maximum depth of the tree? Try\n","different depths and find the best value.\n","\n","Restricting the maximum depth of the tree reduces its complexity, which can help mitigate overfitting. In my experiments, while deeper trees (max depth ≥ 3) achieved perfect training accuracy, they only reached 40% on the test set, whereas a max depth of 2 yielded a more balanced model with 75% training and 80% test accuracy."],"metadata":{"id":"3787tvpvssTw"}},{"cell_type":"markdown","source":["Why does restricting the depth have such a strong effect on the classifier\n","performance?\n","\n","Restricting the depth of a decision tree limits its ability to model very detailed patterns in the training data, which can greatly reduce overfitting. This forced simplification means the model generalizes better to unseen data, even if it sacrifices some training accuracy. At the same time, an unrestricted tree can become overly complex and sensitive to noise, leading to poor performance on new examples."],"metadata":{"id":"1zz9Wm_UtwKi"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# ----------------------------------------\n","# Step 1: Load the training data from Excel\n","# ----------------------------------------\n","train_df = pd.read_excel('Asssignment4_Data.xlsx', sheet_name='Train')\n","\n","# Define the target and identifier columns (adjust as needed)\n","target_col = 'Construction type'\n","id_col = 'House ID'\n","features = train_df.columns.drop([id_col, target_col])\n","\n","# Prepare the training features and labels\n","X_train = train_df[features]\n","y_train = train_df[target_col]\n","\n","# ----------------------------------------\n","# Step 2: Train the Decision Tree Classifier (using best max_depth, e.g., 2)\n","# ----------------------------------------\n","clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# ----------------------------------------\n","# Step 3: Create a new test data point and perform inference\n","# ----------------------------------------\n","# Note: Ensure the feature names exactly match those from the training DataFrame.\n","new_sample = {\n","    \"Local Price\": 9.0384,\n","    \"Bathrooms\": 1,\n","    \"Land Area\": 7.8,\n","    \"Living area\": 1.5,\n","    \"# Garages\": 1.5,\n","    \"# Rooms\": 7,\n","    \"# Bedrooms\": 3,\n","    \"Age of home\": 23\n","}\n","\n","# Convert the new sample to a DataFrame (single row)\n","new_sample_df = pd.DataFrame([new_sample])\n","\n","# Predict the class for the new sample\n","prediction = clf.predict(new_sample_df)\n","print(\"Prediction for the new sample:\", prediction[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmZCH5ymss_X","executionInfo":{"status":"ok","timestamp":1745647890202,"user_tz":300,"elapsed":6,"user":{"displayName":"Miles Smith","userId":"17345950562100669518"}},"outputId":"b70231be-7942-44fd-b986-38a0275bfbe8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction for the new sample: Apartment\n"]}]},{"cell_type":"markdown","source":["For test data point, perform inference on decision tree\n","Local Price = 9.0384\n","Bathrooms = 1\n","Land Area = 7.8\n","Living area = 1.5 #\n","Garages = 1.5 #\n","Rooms = 7\n","Number Bedrooms =3 Age\n","of home = 23\n","\n","\n","The decision tree predicted the new sample as \"Apartment.\" This suggests that even with a max depth of 2, the classifier recognized that the sample’s feature combination is more similar to the apartments in the training set."],"metadata":{"id":"eeYc4x8wujoB"}}]}